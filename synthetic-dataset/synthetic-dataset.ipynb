{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules and classes\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langsmith import Client\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatGoogleGenerativeAI model with specific parameters\n",
    "# This model is used for generating synthetic content based on the given topic and metadata\n",
    "# Parameters:\n",
    "# - model: Specifies the model version to use, in this case, \"gemini-2.0-flash\"\n",
    "# - temperature: Controls the randomness of the output. Lower values make the output more deterministic.\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LangSmith client and pull the prompt for the REACT agent\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"hwchase17/react\", include_model=True)\n",
    "\n",
    "# Initialize the Python REPL tool for generating a structured CSV dataset along with its metadata\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "# Define the tool for the REPL, specifying its name, description, and function\n",
    "repl_tool_csv = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell for generating a structured CSV dataset along with its metadata. Input must be a valid Python command, and all output should be displayed using print(...).\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "# Add the REPL tool to the list of tools for generating the CSV dataset\n",
    "tools_csv = [repl_tool_csv]\n",
    "\n",
    "# Create the REACT agent using the specified LLM, tools, and prompt\n",
    "agent = create_react_agent(llm, tools_csv, prompt)\n",
    "\n",
    "# Initialize the agent executor for the CSV generation, setting verbosity and error handling\n",
    "agent_executor_csv = AgentExecutor(agent=agent, tools=tools_csv, verbose=False, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool for executing Python commands related to the provided CSV dataset, ensuring proper handling of outputs and images\n",
    "repl_tool_paragraph = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell for executing commands strictly related to the provided CSV dataset, such as retrieving statistics, generating plots, and creating summary tables. Input must be a valid Python command, and all output should be displayed using `print(...)`. If the output includes an image file or raw image data, the agent **must not print base64-encoded content or MIME types like `image/png;base64`**â€”instead, it should print only a textual description of the image. Matplotlib must be properly closed after generating plots to prevent rendering issues. Any operations outside of interacting with the given CSV dataset are not supported.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "\n",
    "# Add the REPL tool to the list of tools for generating paragraphs\n",
    "tools_paragraph = [repl_tool_paragraph]\n",
    "\n",
    "# Create the REACT agent using the specified LLM, tools, and prompt for paragraph generation\n",
    "agent = create_react_agent(llm, tools_paragraph, prompt)\n",
    "agent_executor_paragraph = AgentExecutor(agent=agent, tools=tools_paragraph, verbose=False, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic models for chapters, subchapters, and PDF metadata\n",
    "class Chapter(BaseModel):\n",
    "    chapter_index: int = Field(description=\"The chapter number according to the topic.\")\n",
    "    chapter_title: str = Field(description=\"The title of the chapter based on the topic in Bahasa Indonesia.\")\n",
    "    chapter_outline: str = Field(description=\"An outline or key points that need to be written.\")\n",
    "\n",
    "class ChapterList(BaseModel):\n",
    "    chapter_outline_summary: str = Field(description=\"A summary of all chapter outlines.\")\n",
    "    chapter: List[Chapter]\n",
    "\n",
    "class SubChapter(BaseModel):\n",
    "    subchapter_index: int = Field(description=\"The subchapter number within the chapter. Must be an integer.\")\n",
    "    subchapter_title: str = Field(description=\"The title of the subchapter based on the topic in Bahasa Indonesia.\")\n",
    "    subchapter_outline: str = Field(description=\"A structured outline or key points to be covered in the subchapter.\")\n",
    "\n",
    "class SubChapterList(BaseModel):\n",
    "    chapter_index: int = Field(description=\"The chapter number as an integer.\")\n",
    "    chapter_title: str = Field(description=\"The title of the chapter in Bahasa Indonesia.\")\n",
    "    chapter_outline: str = Field(description=\"An outline or key points that need to be written.\")\n",
    "    subchapter_outline_summary: str = Field(description=\"A summarized overview of all subchapter outlines within the chapter.\")\n",
    "    chapter_outline_summary: str = Field(description=\"A summary of all chapter outlines.\")\n",
    "    subchapter: List[SubChapter]\n",
    "\n",
    "class PdfMetadata(BaseModel):\n",
    "    title: str = Field(description=\"The title of the PDF document, extracted from the provided context.\")\n",
    "    author: str = Field(description=\"The author of the PDF document, as specified in the provided context.\")\n",
    "    subject: str = Field(description=\"The subject or main topic of the PDF document, derived from the context.\")\n",
    "    keyword: str = Field(description=\"Keywords associated with the PDF document, extracted from the context. Multiple keywords should be separated by commas.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_csv(topic, agent_executor):\n",
    "    # Define the instruction template for generating a synthetic dataset and its metadata\n",
    "    instruction = \"\"\"\n",
    "    TOPIC:  \n",
    "    {topic}  \n",
    "\n",
    "    Task Overview:  \n",
    "    Generate a structured synthetic dataset based on the given topic. The dataset should be coherent, consistent, and realistic while containing no personally identifiable information or real-world factual data.  \n",
    "\n",
    "    Step 1: Generate a Synthetic CSV Dataset  \n",
    "    Create a CSV file named synthetic-dataset.csv with the following specifications:  \n",
    "\n",
    "    - At least 10 distinct and meaningful columns relevant to the topic.\n",
    "    - 1,000 rows of structured synthetic data.  \n",
    "    - Daily data entries if applicable.  \n",
    "    - If the dataset represents time-series data, ensure it is sorted in descending order, with the most recent date first.  \n",
    "    - Maintain coherence and consistency to ensure realism.  \n",
    "\n",
    "    Step 2: Create Dataset Metadata  \n",
    "    Generate a detailed metadata file named metadata-synthetic-dataset.txt that includes:  \n",
    "\n",
    "    - A comprehensive overview of the dataset.  \n",
    "    - Clear descriptions for each column, explaining its purpose and meaning.  \n",
    "    - Potential use cases to highlight practical applications.  \n",
    "    - Structural insights to enhance understanding of the dataset.  \n",
    "\n",
    "    Do not mention that the data is synthetic or artificially generated. The dataset should appear as a professionally structured, realistic dataset.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the agent executor with the formatted instruction to generate the dataset and metadata\n",
    "    agent_executor.invoke({\"input\":instruction.format(topic=topic)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chapter_outline(llm, topic, metadata_dataset):\n",
    "    # Initialize the JSON output parser with the ChapterList Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=ChapterList)\n",
    "\n",
    "    # Define the instruction template for generating a structured chapter outline\n",
    "    instruction = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    Goal:\n",
    "    Create a structured outline for each chapter of a synthetic (non-factual) document with a total length of 50 pages, based on the given topic.\n",
    "\n",
    "    Content Details:\n",
    "\n",
    "    The document must have a realistic structure but contain fully fabricated information.\n",
    "\n",
    "    This outline will serve as the foundation for developing subchapter outlines in the next stage.\n",
    "\n",
    "    The outline should incorporate a mix of elements such as:\n",
    "\n",
    "    Narrative text\n",
    "\n",
    "    Graphs, charts, and diagrams\n",
    "\n",
    "    Workflows and illustrations\n",
    "\n",
    "    Document Structure & Length:\n",
    "\n",
    "    Total document length: 50 pages.\n",
    "\n",
    "    The number of chapters should be structured proportionally, ensuring each chapter has enough subchapters to meet the total required length.\n",
    "\n",
    "    Each subchapter in a chapter must include at least:\n",
    "\n",
    "    700 words\n",
    "\n",
    "    7 paragraphs\n",
    "\n",
    "    Technical Instructions:\n",
    "\n",
    "    Use only the provided \"Dataset Metadata\" as a reference when constructing the outline.\n",
    "\n",
    "    Ensure the content remains cohesive and contextually relevant, even though it is entirely synthetic.\n",
    "\n",
    "    Avoid nonsensical or meaningless content.\n",
    "\n",
    "    Output Format:\n",
    "    {format_instructions}\n",
    "\n",
    "    Dataset Metadata:\n",
    "    {metadata_dataset}\n",
    "\n",
    "    Topic:\n",
    "    {topic}\n",
    "    \"\"\",\n",
    "        input_variables=[\"topic\", \"metadata_dataset\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Create a chain of the instruction template, LLM, and parser\n",
    "    chain = instruction | llm | parser\n",
    "\n",
    "    # Invoke the chain to generate the chapter outline based on the topic and metadata\n",
    "    chapter_outputs = chain.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"metadata_dataset\": metadata_dataset,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return chapter_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subchapter_outline(llm, topic, metadata_dataset, chapter_output, chapter_outputs):\n",
    "    # Initialize the JSON output parser with the SubChapterList Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=SubChapterList)\n",
    "\n",
    "    # Define the instruction template for generating a structured subchapter outline\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "    Goal:  \n",
    "    This prompt aims to generate a structured outline for each subchapter of a synthetic PDF document with a total length of 50 pages. The subchapter outline will later be used as a foundation for writing the final draft paragraphs.  \n",
    "\n",
    "    You must carefully consider the Chapter Details, Dataset Metadata, and Topic to ensure the outline remains cohesive, contextually relevant, and well-structured, even though the content is entirely synthetic.  \n",
    "\n",
    "    Content Requirements:  \n",
    "    - The document must have a realistic structure but contain fabricated (non-factual) information.  \n",
    "    - The content should include a mix of text, graphs, charts, workflows, and images for a professional and structured presentation.  \n",
    "    - Ensure the text is cohesive, contextually relevant, and well-organized, simulating real-world data while remaining clearly synthetic.  \n",
    "    - Avoid meaningless or nonsensical content.  \n",
    "\n",
    "    Clarification:  \n",
    "    - 50 pages refer to the total length of the document, not the number of subchapters.  \n",
    "    - Each subchapter should contain at least 700 words and 7 paragraphs to help ensure the total document reaches 50 pages.  \n",
    "    - Subchapter indexes must be strictly integers (e.g., 1, 2, 3, etc.). Do not use non-integer values (e.g., 1.1, 2a, etc.).  \n",
    "\n",
    "    Source of Knowledge:  \n",
    "    The only reference for generating the outline and content is the provided Dataset Metadata.  \n",
    "\n",
    "    Chapter Details:  \n",
    "    - Chapter Index: {chapter_index}  \n",
    "    - Chapter Title: {chapter_title}  \n",
    "    - Chapter Outline: {chapter_outline}  \n",
    "    - Overall Chapter Outline: {chapter_outline_summary}  \n",
    "\n",
    "    Format Instructions:  \n",
    "    {format_instructions}  \n",
    "\n",
    "    Dataset Metadata:  \n",
    "    {metadata_dataset}  \n",
    "\n",
    "    Topic:  \n",
    "    {topic}\n",
    "    \"\"\",\n",
    "        input_variables=[\"topic\", \"metadata_dataset\", \"chapter_index\", \"chapter_title\", \"chapter_outline\", \"chapter_outline_summary\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Create a chain of the instruction template, LLM, and parser\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Invoke the chain to generate the subchapter outline based on the topic, metadata, and chapter details\n",
    "    subchapter_outputs = chain.invoke(\n",
    "        {\n",
    "            \"topic\": topic,\n",
    "            \"metadata_dataset\": metadata_dataset,\n",
    "            \"chapter_index\": chapter_output[\"chapter_index\"],\n",
    "            \"chapter_title\": chapter_output[\"chapter_title\"],\n",
    "            \"chapter_outline\": chapter_output[\"chapter_outline\"],\n",
    "            \"chapter_outline_summary\": chapter_outputs[\"chapter_outline_summary\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return subchapter_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paragraph(agent_executor, topic, metadata_dataset, subchapter_output, subchapter_outputs):\n",
    "  # Define the instruction template for generating a detailed and structured subchapter\n",
    "  instruction = \"\"\"\n",
    "Goal:  \n",
    "Generate a detailed and structured subchapter for a 50-page PDF document, ensuring the content is based on the provided chapter details, subchapter details, synthetic dataset, and topic. The output must maintain a realistic format with proper Markdown structuring for headings, tables, and tool-generated images while ensuring the content is coherent, well-organized, and written in Bahasa Indonesia.  \n",
    "\n",
    "Content Requirements:  \n",
    "- The document must have a clear structure and include a mix of text, graphs, charts, workflows, and tables for a structured and professional presentation.  \n",
    "- The writing must be cohesive, contextually relevant, and well-organized, presenting data and insights effectively.  \n",
    "- Use tools **only if you are certain** and can generate code without errors to ensure effectiveness.  \n",
    "- Use tools only when necessary to generate graphs or retrieve data from synthetic-dataset.csv.  \n",
    "- Only include a portion of the synthetic dataset in the contentâ€”do not include the entire dataset.  \n",
    "\n",
    "Graph and Chart Guidelines:  \n",
    "- Ensure generated graphs are human-readable by:  \n",
    "  - Using clear titles, labels, and legends.  \n",
    "  - Formatting axes properly for better comprehension.  \n",
    "  - Using appropriate colors and contrast.  \n",
    "  - Adding grid lines if necessary.  \n",
    "- Ensure that date and time axes are properly formatted and clearly visible by:  \n",
    "  - Using rotated labels (e.g., 45Â° or 90Â°) to prevent overlap.  \n",
    "  - Formatting dates in an easy-to-read manner (e.g., YYYY-MM-DD or HH:MM).  \n",
    "  - Ensuring adequate spacing between labels.  \n",
    "  - Using grid lines and tick marks to improve readability.  \n",
    "  - Adjusting the axis limit to avoid cluttered or compressed labels.  \n",
    "  - Applying Matplotlibâ€™s auto-formatting for time-based data using fig.autofmt_xdate().  \n",
    "\n",
    "Table Guidelines:  \n",
    "- Tables should be used where appropriate to clearly present structured data.  \n",
    "- Only include a relevant portion of the datasetâ€”do not display the entire synthetic dataset.  \n",
    "- Ensure that all generated tables are fully formatted and do not contain unfinished or incomplete structures.  \n",
    "- To prevent truncation when exporting to PDF, tables should have a maximum of **5 columns per table**. If necessary, split larger tables into multiple smaller tables to maintain readability.  \n",
    "\n",
    "Image and File Handling:  \n",
    "- Do not include raw images or base64-encoded images in the output.  \n",
    "- Only include images that are properly saved as PNG files in the /images folder and referenced correctly in Markdown.  \n",
    "\n",
    "Language and Structure:  \n",
    "- The output must be written in Bahasa Indonesia while maintaining the correct structure.  \n",
    "\n",
    "Minimum Requirements:  \n",
    "- At least 5 paragraphs with a minimum of 400 words.  \n",
    "- Graphs (if needed):  \n",
    "  - Generate using Matplotlib **only if you are certain the code will run without errors**.  \n",
    "  - Ensure they are human-readable and properly formatted.  \n",
    "  - Save as PNG files in the /images folder.  \n",
    "  - Embed in Markdown using:  \n",
    "  ![alt text](images/image.png \"Title\")  \n",
    "  - Do not insert images as raw files or base64-encoded data.  \n",
    "- Tables (if needed):  \n",
    "  - Properly format them in Markdown, ensuring completeness. Example:  \n",
    "  | Column 1 | Column 2 | Column 3 | Column 4 | Column 5 |  \n",
    "  |----------|----------|----------|----------|----------|  \n",
    "  | Data 1   | Data 2   | Data 3   | Data 4   | Data 5   |  \n",
    "  | Data 6   | Data 7   | Data 8   | Data 9   | Data 10  |  \n",
    "  - Ensure tables **do not exceed 5 columns per table** to prevent truncation in PDF export.  \n",
    "  - If necessary, split wider tables into multiple smaller tables.  \n",
    "- Tables and images must be properly embedded within the content without being left incomplete or improperly structured.  \n",
    "\n",
    "Format and Output:  \n",
    "- Markdown should only be applied to headings, tables, and correctly stored images.  \n",
    "- Do not start the output with \"Markdown\".  \n",
    "- Subchapter Title Format:  \n",
    "  ### {chapter_index}.{subchapter_index}. {subchapter_title}  \n",
    "- Chapter Title Handling:  \n",
    "  - If the subchapter is the first in the chapter (subchapter_index = 1), prepend the chapter title in the following format:  \n",
    "  ## Bab {chapter_index}. {chapter_title}  \n",
    "  - If the subchapter is not the first one, do not include the chapter title in the output.  \n",
    "- The main body of the text should remain plain text in Bahasa Indonesia.  \n",
    "\n",
    "Source of Knowledge:  \n",
    "The content must be based only on the following sources:  \n",
    "1. Synthetic Dataset (synthetic-dataset.csv) â€“ A dataset used for generating structured information in the document. Only a portion of this dataset should be included in the content.  \n",
    "2. Synthetic Dataset Metadata ({metadata_dataset}) â€“ Metadata that describes the structure, variables, and attributes of the dataset.  \n",
    "\n",
    "Subchapter Details:  \n",
    "- Subchapter Index: {subchapter_index}  \n",
    "- Subchapter Title: {subchapter_title}  \n",
    "- Subchapter Outline: {subchapter_outline}  \n",
    "\n",
    "Chapter Context:  \n",
    "- Chapter Index: {chapter_index}  \n",
    "- Chapter Title: {chapter_title} (Only included when subchapter_index = 1)  \n",
    "- Summarized Overview of Subchapter Outlines: {subchapter_outline_summary}  \n",
    "- Topic: {topic}  \n",
    "  \"\"\"\n",
    "\n",
    "  # Invoke the agent executor to generate the paragraph based on the formatted instruction\n",
    "  paragprah_output = agent_executor.invoke({\"input\":instruction.format(\n",
    "    topic= topic,\n",
    "    metadata_dataset=metadata_dataset,\n",
    "    subchapter_index=subchapter_output[\"subchapter_index\"],\n",
    "    subchapter_title=subchapter_output[\"subchapter_title\"],\n",
    "    subchapter_outline=subchapter_output[\"subchapter_outline\"],\n",
    "    chapter_index=subchapter_outputs[\"chapter_index\"],\n",
    "    chapter_title=subchapter_outputs[\"chapter_title\"],\n",
    "    subchapter_outline_summary=subchapter_outputs[\"subchapter_outline_summary\"],\n",
    "  )})\n",
    "\n",
    "  return paragprah_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pdf_metadata(llm, chapter_outputs):\n",
    "    # Initialize the JSON output parser with the PdfMetadata Pydantic model\n",
    "    parser = JsonOutputParser(pydantic_object=PdfMetadata)\n",
    "\n",
    "    # Define the instruction template for generating PDF metadata\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "Based on the provided context, generate the title, author, subject, and keywords for the PDF document metadata in Bahasa Indonesia.\n",
    "Ensure the metadata is relevant to the content without indicating that it is fictional.\n",
    "\n",
    "Format Instructions:  \n",
    "{format_instructions}  \n",
    "\n",
    "Context:\n",
    "{chapter_outputs}\n",
    "    \"\"\",\n",
    "        input_variables=[\"chapter_outputs\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Create a chain of the instruction template, LLM, and parser\n",
    "    chain = prompt | llm | parser\n",
    "\n",
    "    # Invoke the chain to generate the PDF metadata based on the chapter outputs\n",
    "    pdf_metadata = chain.invoke(\n",
    "        {\n",
    "            \"chapter_outputs\": chapter_outputs,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return pdf_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    }
   ],
   "source": [
    "# Define the topic for the synthetic dataset and generate the dataset CSV using the specified agent executor\n",
    "topic = \"Wakanda Country Statistical Report - The Richest Country Nobody Knows About\"\n",
    "generate_dataset_csv(topic, agent_executor_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL file generated: ../synthetic-dataset.sql\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file into DataFrame\n",
    "csv_file = \"synthetic-dataset.csv\"  # Change this to your actual CSV file path\n",
    "table_name = \"my_table\"  # Change this to your desired table name\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Generate SQL statements\n",
    "sql_file = \"../synthetic-dataset.sql\"\n",
    "with open(sql_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"CREATE TABLE {table_name} (\\n\")\n",
    "    f.write(\",\\n\".join([f\"    {col} TEXT\" for col in df.columns]))\n",
    "    f.write(\"\\n);\\n\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        values = \"', '\".join(map(str, row.tolist()))\n",
    "        f.write(f\"INSERT INTO {table_name} VALUES ('{values}');\\n\")\n",
    "\n",
    "print(f\"SQL file generated: {sql_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the metadata for the synthetic dataset from the file and store it in the variable 'metadata_dataset'\n",
    "with open('metadata-synthetic-dataset.txt', 'r') as file:\n",
    "    metadata_dataset = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Pendahuluan: Wakanda yang Tersembunyi - Gambaran Statistik\n",
      "2 Ekonomi Wakanda: Analisis PDB dan Produksi Vibranium\n",
      "3 Investasi Sosial: Pendidikan dan Kesehatan di Wakanda\n",
      "4 Kemajuan Teknologi: Investasi dan Dampak di Wakanda\n",
      "5 Manajemen Sumber Daya: Produksi Energi dan Keberlanjutan di Wakanda\n",
      "6 Tantangan dan Peluang: Masa Depan Statistik Wakanda\n"
     ]
    }
   ],
   "source": [
    "# Generate the chapter outline based on the topic and metadata\n",
    "chapter_outputs = generate_chapter_outline(llm, topic, metadata_dataset)\n",
    "\n",
    "# Print the index and title of each chapter in the generated outline\n",
    "for i in range(len(chapter_outputs[\"chapter\"])):\n",
    "    print(str(chapter_outputs[\"chapter\"][i][\"chapter_index\"])+\" \"+chapter_outputs[\"chapter\"][i][\"chapter_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Pendahuluan: Wakanda yang Tersembunyi - Gambaran Statistik\n",
      "\t1 Wakanda: Anomali Geopolitik dan Statistik\n",
      "\t2 Tinjauan Ekonomi Wakanda: PDB dan Dominasi Vibranium\n",
      "\t3 Indikator Pembangunan Sosial: Pendidikan dan Kesehatan di Wakanda\n",
      "\t4 Kemajuan Teknologi: Dampak Vibranium pada Inovasi Wakanda\n",
      "\t5 Metodologi dan Sumber Data: Memahami Statistik Wakanda\n",
      "2 Ekonomi Wakanda: Analisis PDB dan Produksi Vibranium\n",
      "\t1 Tinjauan Umum PDB Wakanda: Pertumbuhan dan Tren Historis\n",
      "\t2 Peran Produksi Vibranium dalam Ekonomi Wakanda\n",
      "\t3 Distribusi Kekayaan dan Tingkat Kemiskinan di Wakanda\n",
      "\t4 Perbandingan Ekonomi Wakanda dengan Negara Lain\n",
      "\t5 Model Ekonomi Hipotetis: Dampak Vibranium pada Ekonomi Wakanda\n",
      "\t6 Investasi Asing Langsung (FDI) dan Pengaruhnya terhadap PDB Wakanda\n",
      "\t7 Diversifikasi Ekonomi Wakanda: Menuju Keberlanjutan di Luar Vibranium\n",
      "3 Investasi Sosial: Pendidikan dan Kesehatan di Wakanda\n",
      "\t1 Sistem Pendidikan Wakanda: Struktur dan Investasi\n",
      "\t2 Sistem Kesehatan Wakanda: Akses dan Kualitas\n",
      "\t3 Disparitas Regional dalam Pembangunan Sosial di Wakanda\n",
      "\t4 Dampak Investasi Sosial pada Kesempatan Ekonomi\n",
      "\t5 Studi Kasus: Transformasi Hidup Melalui Investasi Sosial\n",
      "\t6 Perbandingan Internasional: Wakanda dalam Konteks Global\n",
      "\t7 Masa Depan Investasi Sosial di Wakanda: Tantangan dan Peluang\n",
      "4 Kemajuan Teknologi: Investasi dan Dampak di Wakanda\n",
      "\t1 Sejarah Investasi Teknologi di Wakanda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t2 Tren Investasi Teknologi Saat Ini di Wakanda\n",
      "\t3 Dampak Teknologi pada Sektor Manufaktur Wakanda\n",
      "\t4 Peran Teknologi dalam Memajukan Pertanian Wakanda\n",
      "\t5 Teknologi dalam Produksi dan Distribusi Energi Wakanda\n",
      "\t6 Pertimbangan Etis dalam Kemajuan Teknologi di Wakanda\n",
      "\t7 Peta Jalan Teknologi untuk Masa Depan Wakanda\n",
      "5 Manajemen Sumber Daya: Produksi Energi dan Keberlanjutan di Wakanda\n",
      "\t1 Tren Historis Produksi Energi di Wakanda\n",
      "\t2 Sumber Energi Wakanda: Diversifikasi dan Inovasi\n",
      "\t3 Praktik Keberlanjutan: Inisiatif dan Kebijakan Wakanda\n",
      "\t4 Dampak Lingkungan dari Manajemen Sumber Daya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t5 Peran Vibranium dalam Produksi Energi Berkelanjutan\n",
      "\t6 Analisis Tren Produksi Energi: Grafik dan Bagan\n",
      "\t7 Rencana Keberlanjutan Wakanda: Visi untuk Masa Depan\n",
      "6 Tantangan dan Peluang: Masa Depan Statistik Wakanda\n",
      "\t1 Analisis Risiko Ekonomi Wakanda\n",
      "\t2 Peluang Pengembangan Sosial dan Kesejahteraan Warga\n",
      "\t3 Manajemen Sumber Daya Vibranium yang Berkelanjutan\n",
      "\t4 Peran Data dan Statistik dalam Kebijakan Publik\n",
      "\t5 Skenario Masa Depan Wakanda: Analisis Hipotesis\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the generated markdown content\n",
    "markdown_content_list = []\n",
    "\n",
    "# Iterate over each chapter in the generated chapter outline\n",
    "for i in range(len(chapter_outputs[\"chapter\"])):\n",
    "    # Generate the subchapter outline for the current chapter\n",
    "    subchapter_outputs = generate_subchapter_outline(llm, topic, metadata_dataset, chapter_outputs[\"chapter\"][i], chapter_outputs)\n",
    "\n",
    "    # Print the index and title of the current chapter\n",
    "    print(str(chapter_outputs[\"chapter\"][i][\"chapter_index\"])+\" \"+chapter_outputs[\"chapter\"][i][\"chapter_title\"])\n",
    "\n",
    "    # Iterate over each subchapter in the generated subchapter outline\n",
    "    for j in range(len(subchapter_outputs[\"subchapter\"])):\n",
    "        # Generate the paragraph content for the current subchapter\n",
    "        paragprah_output = generate_paragraph(agent_executor_paragraph, topic, metadata_dataset, subchapter_outputs[\"subchapter\"][j], subchapter_outputs)\n",
    "\n",
    "        # Print the index and title of the current subchapter\n",
    "        print(\"\\t\"+str(subchapter_outputs[\"subchapter\"][j][\"subchapter_index\"])+\" \"+subchapter_outputs[\"subchapter\"][j][\"subchapter_title\"])\n",
    "\n",
    "        # Append the generated paragraph content to the markdown content list\n",
    "        markdown_content_list.append(paragprah_output[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all markdown content into a single string, removing any code block markers (```)\n",
    "markdown_content = \"\\n\".join(markdown_content_list).replace(\"```\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the combined markdown content to a file named \"synthetic-dataset.md\"\n",
    "with open(\"synthetic-dataset.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(markdown_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the PDF metadata based on the chapter outputs using the LLM\n",
    "pdf_metadata = generate_pdf_metadata(llm, chapter_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markdown_pdf import MarkdownPdf, Section\n",
    "\n",
    "# Initialize the MarkdownPdf object\n",
    "pdf = MarkdownPdf()\n",
    "\n",
    "# Set PDF metadata using the generated metadata\n",
    "pdf.meta[\"title\"] = pdf_metadata[\"title\"]\n",
    "pdf.meta[\"author\"] = pdf_metadata[\"author\"]\n",
    "pdf.meta[\"subject\"] = pdf_metadata[\"subject\"]\n",
    "pdf.meta[\"keywords\"] = pdf_metadata[\"keyword\"]\n",
    "\n",
    "# Define CSS for justified text and centered table\n",
    "css = \"\"\"\n",
    "body { text-align: justify; }\n",
    "table { border: 1px solid black; border-collapse: collapse; margin: auto; }\n",
    "th, td { border: 1px solid black; padding: 5px; text-align: center; }\n",
    "\"\"\"\n",
    "\n",
    "# Add Title and Author section on the first page\n",
    "title_author_section = Section(f\"# {pdf_metadata['title']}\\n\\n**Author:** {pdf_metadata['author']}\\n\\n\")\n",
    "pdf.add_section(title_author_section)\n",
    "\n",
    "# Add main content section with custom CSS\n",
    "pdf.add_section(Section(markdown_content), user_css=css)\n",
    "\n",
    "# Save the generated PDF to a file\n",
    "pdf.save(\"../synthetics-dataset.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
