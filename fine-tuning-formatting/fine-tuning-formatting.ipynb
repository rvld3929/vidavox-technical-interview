{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters extracted: 156911\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF\n",
    "pdf_path = \"../synthetics-dataset.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# Extract text from PDF\n",
    "documents = loader.load()\n",
    "\n",
    "# Combine text from all pages\n",
    "pdf_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "print(f\"Total characters extracted: {len(pdf_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Analisis Statistik Wakanda: Ekonomi,\n",
      "Sosial, dan Teknologi\n",
      "Author: Tim Analisis Wakanda\n",
      "Bab 1. Pendahuluan: Wakanda yang Tersembunyi -\n",
      "Gambaran Statistik\n",
      "1.1. Wakanda: Anomali Geopolitik dan Statistik\n",
      "Subbab ini akan membahas posisi geopolitik unik Wakanda sebagai negara yang tersembunyi\n",
      "dan anomali statistiknya dibandingkan dengan negara lain. Wakanda, sebuah negara yang\n",
      "kaya akan sumber daya alam dan teknologi maju, telah lama memilih untuk mengisolasi diri\n",
      "dari dunia luar. Keputusan ini bukan tanpa alasan; perlindungan budaya, sumber daya\n",
      "Vibranium, dan kemajuan teknologi yang tak tertandingi menjadi prioritas utama. Isolasi ini\n",
      "memiliki implikasi yang signiﬁkan terhadap data statistik Wakanda, menjadikannya sulit\n",
      "untuk dibandingkan dengan negara-negara lain yang lebih terbuka dan terintegrasi secara\n",
      "global.\n",
      "Wakanda sering dianggap sebagai 'statistical outlier' karena kemajuan teknologi dan sumber\n",
      "daya uniknya, Vibranium. Vibranium, logam langka dengan sifat luar biasa, telah\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  \n",
    "    chunk_overlap=100,  \n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    ")\n",
    "\n",
    "# Split text into chunks\n",
    "chunks = text_splitter.split_text(pdf_text)\n",
    "\n",
    "# Display first few chunks\n",
    "for i, chunk in enumerate(chunks[:1]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class InstructionFineTuneData(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for formatting instruction-tuning dataset for fine-tuning LLMs.\n",
    "    Ensures consistency and validation across different foundational models.\n",
    "    \"\"\"\n",
    "    \n",
    "    instruction: str = Field(description=\"The prompt or question given to the model.\")\n",
    "    input: Optional[str] = Field(None, description=\"Optional input context or passage relevant to the instruction.\")\n",
    "    output: str = Field(description=\"The expected response from the model.\")\n",
    "\n",
    "class InstructionFineTuneDataset(BaseModel):\n",
    "    \"\"\"\n",
    "    Schema for a list of instruction fine-tuning data instances.\n",
    "    \"\"\"\n",
    "    data: List[InstructionFineTuneData] = Field(..., description=\"List of instruction fine-tuning data samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=InstructionFineTuneDataset)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an AI assistant trained to create instruction fine-tuning datasets.\n",
    "Given the following text chunk, generate multiple instruction-response pairs suitable for fine-tuning a language model **only if you are certain about the validity of the generated instructions and responses**.\n",
    "\n",
    "Text Chunk:\n",
    "{chunk}\n",
    "\n",
    "Format the output as a JSON list where each item contains the following fields:\n",
    "- instruction: Each instruction must be clear, concise, and directly derived from the text chunk.\n",
    "- input: The input should include the provided text chunk only if necessary—otherwise, leave it empty. Remove unnecessary newlines from the input.\n",
    "- output: The output must be a well-structured and accurate response to the instruction.\n",
    "\n",
    "Ensure that the language of the generated instructions and responses matches the language of the input text chunk.\n",
    "\n",
    "**Format Instructions:**  \n",
    "{format_instructions}  \n",
    "\"\"\",\n",
    "    input_variables=[\"chunk\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm | parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:19<00:00,  3.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process each chunk\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for chunk in tqdm(chunks[:5]):\n",
    "    output = chain.invoke({\"chunk\":chunk})\n",
    "    dataset.extend(output[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
